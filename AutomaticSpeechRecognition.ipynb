{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminojagh/HFLLM/blob/main/AutomaticSpeechRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xGWkdPm18cE"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets evaluate accelerate torchcodec jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vVGeOCE18cU"
      },
      "outputs": [],
      "source": [
        "user_name = \"amin-oj\"\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYIwbLC818cM"
      },
      "source": [
        "# Automatic speech recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlPWA5dA18cT"
      },
      "source": [
        "Automatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users every day, and there are many other useful user-facing applications like live captioning and note-taking during meetings.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Fine-tune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to transcribe audio to text.\n",
        "2. Use your fine-tuned model for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6XJ463z18cV"
      },
      "source": [
        "## Load MInDS-14 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzcTi-D718cY"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:100]\")\n",
        "minds = minds.train_test_split(test_size=0.2)\n",
        "print(minds)\n",
        "minds = minds.remove_columns([\"english_transcription\", \"intent_class\", \"lang_id\"])\n",
        "minds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2632BlX18ce"
      },
      "source": [
        "There are two fields:\n",
        "\n",
        "- `audio`: a 1-dimensional `array` of the speech signal that must be called to load and resample the audio file.\n",
        "- `transcription`: the target text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMz99JBi18ce"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFPx94p718ce"
      },
      "source": [
        "The next step is to load a Wav2Vec2 processor to process the audio signal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi7MFgAi18ce"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "checkpoint = \"facebook/wav2vec2-base\"\n",
        "processor = AutoProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9hQwtA818cf"
      },
      "source": [
        "The MInDS-14 dataset has a sampling rate of 8000Hz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16000Hz to use the pretrained Wav2Vec2 model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw_cEbih18cf"
      },
      "outputs": [],
      "source": [
        "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "minds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZDnamSb18cf"
      },
      "source": [
        "As you can see in the `transcription` above, the text contains a mix of uppercase and lowercase characters. The Wav2Vec2 tokenizer is only trained on uppercase characters so you'll need to make sure the text matches the tokenizer's vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoZElcJ818cf"
      },
      "outputs": [],
      "source": [
        "def uppercase(example):\n",
        "    return {\"transcription\": example[\"transcription\"].upper()}\n",
        "    # TODO: we're just returning the transcription.\n",
        "    # what about the other fields?\n",
        "\n",
        "minds = minds.map(uppercase)\n",
        "minds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIWLlXe818cg"
      },
      "source": [
        "Now create a preprocessing function that:\n",
        "\n",
        "1. Calls the `audio` column to load and resample the audio file.\n",
        "2. Extracts the `input_values` from the audio file and tokenize the `transcription` column with the processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwRBgH7718cg"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch = processor(audio[\"array\"],\n",
        "                      sampling_rate=audio[\"sampling_rate\"],\n",
        "                      text=batch[\"transcription\"])\n",
        "    batch[\"input_length\"] = len(batch[\"input_values\"][0])\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfIcvwJ118cg"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by increasing the number of processes with the `num_proc` parameter. Remove the columns you don't need with the [remove_columns](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.remove_columns) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maIFZ2uN18ch"
      },
      "outputs": [],
      "source": [
        "encoded_minds = minds.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=minds.column_names[\"train\"],\n",
        "    # remove ['path', 'audio', 'transcription']\n",
        "    num_proc=8\n",
        ")\n",
        "\n",
        "encoded_minds.column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VXQgmFZ18ch"
      },
      "source": [
        "ðŸ¤— Transformers doesn't have a data collator for ASR, so you'll need to adapt the [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding) to create a batch of examples. It'll also dynamically pad your text and labels to the length of the longest element in its batch (instead of the entire dataset) so they are a uniform length.\n",
        "\n",
        "<font color=\"lightgreen\">While it is possible to pad your text in the `tokenizer` function by setting `padding=True`, dynamic padding is more efficient.</font>\n",
        "\n",
        "Unlike other data collators, this specific data collator needs to apply a different padding method to `input_values` and `labels`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1XLpCtm18ci"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    processor: AutoProcessor\n",
        "    padding: Union[bool, str] = \"longest\"\n",
        "\n",
        "    def __call__(\n",
        "        self, features: list[dict[str, Union[list[int], torch.Tensor]]]\n",
        "    ) -> dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be\n",
        "        # of different lengths and need different padding methods\n",
        "        input_features = [\n",
        "            {\"input_values\": feature[\"input_values\"][0]}\n",
        "            for feature in features\n",
        "        ]\n",
        "        label_features = [\n",
        "            {\"input_ids\": feature[\"labels\"]} for feature in features\n",
        "        ]\n",
        "        # TODO: does \"input_ids\" have a special meaning here?\n",
        "        # based on ??processor.pad:\n",
        "        # the processor.feature_extractor.pad is applied on input_features\n",
        "        # the processor.tokenizer.pad is applied on labels\n",
        "\n",
        "        batch = self.processor.pad(input_features,\n",
        "                                   padding=self.padding,\n",
        "                                   return_tensors=\"pt\")\n",
        "\n",
        "        labels_batch = self.processor.pad(labels=label_features,\n",
        "                                          padding=self.padding,\n",
        "                                          return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"]\\\n",
        "                  .masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorCTCWithPadding(\n",
        "    processor=processor,\n",
        "    padding=\"longest\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2A-ZPoZ18ci"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q0GQ3UI18cj"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load an evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [word error rate](https://huggingface.co/spaces/evaluate-metric/wer) (WER) metric (refer to the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about loading and computing metrics):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTV6lQEV18cj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "wer = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer_score = wer.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer_score}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzDoUHxG18ck"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhhXpXs018cv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCTC, TrainingArguments, Trainer\n",
        "model = AutoModelForCTC.from_pretrained(\n",
        "    checkpoint,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "model_name = checkpoint.split(\"/\")[-1]\n",
        "task = \"ASR\"\n",
        "data_id = \"minds14\"\n",
        "ckpt_name = f\"{model_name}-finetuned-{task}-{data_id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsOLC7A818cw"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=ckpt_name,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=150,\n",
        "    max_steps=500,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    group_by_length=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    report_to = 'none'\n",
        "    # to disable w&b\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_minds[\"train\"],\n",
        "    eval_dataset=encoded_minds[\"test\"],\n",
        "    processing_class=processor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "4iOnugCZ0wHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqR3Eo_118cw"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRtADbet18cw"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog [post](https://huggingface.co/blog/fine-tune-wav2vec2-english) for English ASR and this [post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) for multilingual ASR.\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-fNKng_18cw"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor\n",
        "from transformers import AutoModelForCTC\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
        "audio_file = dataset[0][\"audio\"]['array']\n",
        "\n",
        "\n",
        "# 1-simpleset way using `pipeline`\n",
        "\n",
        "# our own model might generate empty text. use this model instead\n",
        "model_name = \"openai/whisper-base\"\n",
        "\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=model_name)\n",
        "print(transcriber(audio_file))\n",
        "\n",
        "# 2-Manually Load a processor to preprocess the audio file and transcription\n",
        "# and return the `input` as PyTorch tensors:\n",
        "\n",
        "model_name = f\"{user_name}/{ckpt_name}\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForCTC.from_pretrained(model_name)\n",
        "\n",
        "inputs = processor(\n",
        "    audio_file, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "6oTudIDkC-1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}